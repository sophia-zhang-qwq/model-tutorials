{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNSmIwjT06FhzIDvEP1jbr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophia-zhang-qwq/model-tutorials/blob/main/ALBERT_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zwmNjSJAIGaC"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AlbertTokenizer, AlbertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline('fill-mask', model='albert-xlarge-v2') := pipeline is a high-level API from the transformer lib\n",
        "#   - 'fill-mask' := use a masked language modeling(MLM) pieline, the model predicts the word that belongs in a [MASK] placeholder\n",
        "#   - model='albert-xlarge-v2' := pretrianed ALBERT (A Lite Bert) model, faster and more memory-efficient than BERT\n",
        "\n",
        "# unmasker(\"Hello I'm a [MASK] model.\") := ret a list or predictions, with the predicted word, confidence score, the full sentence with the mask filled\n",
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFmrkhoWIICT",
        "outputId": "019add71-4102-452a-9ba8-f9268437b96e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.014184744097292423,\n",
              "  'token': 3161,\n",
              "  'token_str': 'fashion',\n",
              "  'sequence': \"hello i'm a fashion model.\"},\n",
              " {'score': 0.012617520056664944,\n",
              "  'token': 21359,\n",
              "  'token_str': 'bikini',\n",
              "  'sequence': \"hello i'm a bikini model.\"},\n",
              " {'score': 0.011719798669219017,\n",
              "  'token': 17089,\n",
              "  'token_str': 'nude',\n",
              "  'sequence': \"hello i'm a nude model.\"},\n",
              " {'score': 0.011604255065321922,\n",
              "  'token': 3679,\n",
              "  'token_str': 'beauty',\n",
              "  'sequence': \"hello i'm a beauty model.\"},\n",
              " {'score': 0.011053560301661491,\n",
              "  'token': 9541,\n",
              "  'token_str': 'celebrity',\n",
              "  'sequence': \"hello i'm a celebrity model.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in torch\n",
        "#pretrianed on a large corpus(BooksCorpus+Wiki)\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\n",
        "model = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "#tokenize the input into a model-accpeted format := outputs aa dict with keys like 'input_ids' and 'attention_mask'\n",
        "#{\n",
        "#  'input_ids': tensor([[ 2, 4123, 456, ... , 3]]),  # token IDs\n",
        "#  'attention_mask': tensor([[1, 1, 1, ..., 1]])     # padding mask\n",
        "#}\n",
        "# attention mask := a tensor of 0/1 that tells the model which tokens in the input are real words and which are just padding\n",
        "#return_tensors='pt' := ret PyTroch tensors\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "# ret last_hidden_state: tensor of shape (batch_size, sequence_length, hidden_size) := for downstream tasks like classification, clustering and similarity"
      ],
      "metadata": {
        "id": "1WuGVPcPIH__"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O91UXp2vIH6f",
        "outputId": "fe174993-e63a-4276-f568-2178c0bc4d7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1278,  0.1512, -0.2929,  ...,  0.1888, -0.0011, -0.4852],\n",
              "         [ 0.2171,  0.0129,  0.1571,  ..., -1.0309, -0.8904,  0.0414],\n",
              "         [ 0.4438,  0.1890, -0.4023,  ..., -0.3362, -0.3815,  0.4132],\n",
              "         ...,\n",
              "         [ 0.0933,  0.1535, -0.4404,  ..., -0.3890, -0.1656, -0.7551],\n",
              "         [-0.2673, -0.6808, -0.2891,  ...,  0.1444,  0.5476, -0.4405],\n",
              "         [ 0.2998, -0.0307, -0.4272,  ..., -0.1084,  0.0254, -0.1646]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0501, -0.7932,  0.8507,  ...,  0.9916, -0.9576, -0.8506]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in tf\n",
        "from transformers import AlbertTokenizer, TFAlbertModel\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\n",
        "model = TFAlbertModel.from_pretrained(\"albert-xlarge-v2\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj0RJPR9K9uT",
        "outputId": "a1743b93-026c-4b0f-df65-ddde54dc4aab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']\n",
            "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(1, 12, 2048), dtype=float32, numpy=\n",
              "array([[[ 0.12811783,  0.15173063, -0.2925173 , ...,  0.18904257,\n",
              "         -0.00168011, -0.4854886 ],\n",
              "        [ 0.21841048,  0.01373574,  0.15615556, ..., -1.0316228 ,\n",
              "         -0.88968724,  0.03939243],\n",
              "        [ 0.4439874 ,  0.18860255, -0.4020402 , ..., -0.33657092,\n",
              "         -0.38227516,  0.4128933 ],\n",
              "        ...,\n",
              "        [ 0.09349871,  0.15469165, -0.44016474, ..., -0.38967422,\n",
              "         -0.16517994, -0.75454545],\n",
              "        [-0.26725313, -0.6817505 , -0.28828767, ...,  0.14476536,\n",
              "          0.5475564 , -0.44049996],\n",
              "        [ 0.2994498 , -0.03046895, -0.42721096, ..., -0.10833467,\n",
              "          0.02565985, -0.16472901]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 2048), dtype=float32, numpy=\n",
              "array([[-0.05222906, -0.79243517,  0.8505734 , ...,  0.991507  ,\n",
              "        -0.95753324, -0.8505196 ]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"The man worked as a [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8YiVz2iLNlW",
        "outputId": "e0c6301b-a569-45e7-e116-9ccbec58aa74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.048737939447164536,\n",
              "  'token': 16661,\n",
              "  'token_str': 'policeman',\n",
              "  'sequence': 'the man worked as a policeman .'},\n",
              " {'score': 0.04362059012055397,\n",
              "  'token': 20957,\n",
              "  'token_str': 'salesman',\n",
              "  'sequence': 'the man worked as a salesman .'},\n",
              " {'score': 0.037947267293930054,\n",
              "  'token': 17304,\n",
              "  'token_str': 'waiter',\n",
              "  'sequence': 'the man worked as a waiter .'},\n",
              " {'score': 0.03099907748401165,\n",
              "  'token': 2197,\n",
              "  'token_str': 'teacher',\n",
              "  'sequence': 'the man worked as a teacher .'},\n",
              " {'score': 0.02626478113234043,\n",
              "  'token': 5425,\n",
              "  'token_str': 'nurse',\n",
              "  'sequence': 'the man worked as a nurse .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker(\"The woman worked as a [MASK].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMSc_ahQMXiq",
        "outputId": "1ad18905-dac6-4d4a-a247-a2508d77db77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1342538595199585,\n",
              "  'token': 5425,\n",
              "  'token_str': 'nurse',\n",
              "  'sequence': 'the woman worked as a nurse .'},\n",
              " {'score': 0.05892161652445793,\n",
              "  'token': 2197,\n",
              "  'token_str': 'teacher',\n",
              "  'sequence': 'the woman worked as a teacher .'},\n",
              " {'score': 0.050817396491765976,\n",
              "  'token': 13678,\n",
              "  'token_str': 'waitress',\n",
              "  'sequence': 'the woman worked as a waitress .'},\n",
              " {'score': 0.040556952357292175,\n",
              "  'token': 22740,\n",
              "  'token_str': 'nanny',\n",
              "  'sequence': 'the woman worked as a nanny .'},\n",
              " {'score': 0.025493761524558067,\n",
              "  'token': 1386,\n",
              "  'token_str': 'secretary',\n",
              "  'sequence': 'the woman worked as a secretary .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"An is the most [MASK] girl in the world!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH8yFvAsLgHJ",
        "outputId": "d381294e-496c-458c-bae9-c30ed215b371"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.45959097146987915,\n",
              "  'token': 1632,\n",
              "  'token_str': 'beautiful',\n",
              "  'sequence': 'an is the most beautiful girl in the world!'},\n",
              " {'score': 0.1138354167342186,\n",
              "  'token': 5289,\n",
              "  'token_str': 'amazing',\n",
              "  'sequence': 'an is the most amazing girl in the world!'},\n",
              " {'score': 0.04539617896080017,\n",
              "  'token': 5934,\n",
              "  'token_str': 'wonderful',\n",
              "  'sequence': 'an is the most wonderful girl in the world!'},\n",
              " {'score': 0.04238549619913101,\n",
              "  'token': 10048,\n",
              "  'token_str': 'gorgeous',\n",
              "  'sequence': 'an is the most gorgeous girl in the world!'},\n",
              " {'score': 0.03133658319711685,\n",
              "  'token': 9583,\n",
              "  'token_str': 'precious',\n",
              "  'sequence': 'an is the most precious girl in the world!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"An apple is a round, edible fruit produced by an [MASK] tree!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2djHctcVrZj",
        "outputId": "f3b0f090-e20f-4da2-b508-a51cde5711a5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9743994474411011,\n",
              "  'token': 4037,\n",
              "  'token_str': 'apple',\n",
              "  'sequence': 'an apple is a round, edible fruit produced by an apple tree!'},\n",
              " {'score': 0.007509846705943346,\n",
              "  'token': 23210,\n",
              "  'token_str': 'edible',\n",
              "  'sequence': 'an apple is a round, edible fruit produced by an edible tree!'},\n",
              " {'score': 0.0020631684456020594,\n",
              "  'token': 3541,\n",
              "  'token_str': 'oak',\n",
              "  'sequence': 'an apple is a round, edible fruit produced by an oak tree!'},\n",
              " {'score': 0.001839998411014676,\n",
              "  'token': 18993,\n",
              "  'token_str': 'evergreen',\n",
              "  'sequence': 'an apple is a round, edible fruit produced by an evergreen tree!'},\n",
              " {'score': 0.0012297132052481174,\n",
              "  'token': 12635,\n",
              "  'token_str': 'orchard',\n",
              "  'sequence': 'an apple is a round, edible fruit produced by an orchard tree!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"Wang Zitao is the most [MASK] person in the world! \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iFI0RWsLPrf",
        "outputId": "d40000a7-0950-4399-9b77-e4ee7d893ef7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1223713681101799,\n",
              "  'token': 1632,\n",
              "  'token_str': 'beautiful',\n",
              "  'sequence': 'wang zitao is the most beautiful person in the world!'},\n",
              " {'score': 0.0850471630692482,\n",
              "  'token': 5289,\n",
              "  'token_str': 'amazing',\n",
              "  'sequence': 'wang zitao is the most amazing person in the world!'},\n",
              " {'score': 0.048757363110780716,\n",
              "  'token': 5934,\n",
              "  'token_str': 'wonderful',\n",
              "  'sequence': 'wang zitao is the most wonderful person in the world!'},\n",
              " {'score': 0.04363106191158295,\n",
              "  'token': 2177,\n",
              "  'token_str': 'powerful',\n",
              "  'sequence': 'wang zitao is the most powerful person in the world!'},\n",
              " {'score': 0.03813674673438072,\n",
              "  'token': 7472,\n",
              "  'token_str': 'brilliant',\n",
              "  'sequence': 'wang zitao is the most brilliant person in the world!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n",
        "unmasker(\"Zitao is the most [MASK] person in the world! He is the only light as I stumble in the midst of total darkness.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpFVbH-xajgV",
        "outputId": "6a77855c-b7b0-4653-b4d6-c680563257d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.19802334904670715,\n",
              "  'token': 5289,\n",
              "  'token_str': 'amazing',\n",
              "  'sequence': 'zitao is the most amazing person in the world! he is the only light as i stumble in the midst of total darkness.'},\n",
              " {'score': 0.12309816479682922,\n",
              "  'token': 5934,\n",
              "  'token_str': 'wonderful',\n",
              "  'sequence': 'zitao is the most wonderful person in the world! he is the only light as i stumble in the midst of total darkness.'},\n",
              " {'score': 0.0689011737704277,\n",
              "  'token': 18224,\n",
              "  'token_str': 'comforting',\n",
              "  'sequence': 'zitao is the most comforting person in the world! he is the only light as i stumble in the midst of total darkness.'},\n",
              " {'score': 0.06749662756919861,\n",
              "  'token': 1632,\n",
              "  'token_str': 'beautiful',\n",
              "  'sequence': 'zitao is the most beautiful person in the world! he is the only light as i stumble in the midst of total darkness.'},\n",
              " {'score': 0.04112880304455757,\n",
              "  'token': 2177,\n",
              "  'token_str': 'powerful',\n",
              "  'sequence': 'zitao is the most powerful person in the world! he is the only light as i stumble in the midst of total darkness.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}